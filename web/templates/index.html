<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Instructions</title>

    <style>
        /* The Overlay (background) */
        .overlay {
        position: fixed; /* Sit on top of the page content */
        display: none; /* Hidden by default */
        width: 100%; /* Full width */
        height: 100%; /* Full height */
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background-color: white; /* Solid white background */
        z-index: 2; /* Specify a stack order in case you're using a different order for other elements */
        cursor: pointer; /* Add a pointer on hover */
        }

      
        /* The Overlay Content */
        .overlay-content {
          position: absolute;
          top: 50%;
          left: 50%;
          transform: translate(-50%, -50%);
          text-align: center;
          font-size: 30px; /* Larger text */
        }
      
        /* The Close Button */
        .overlay-close {
          position: absolute;
          top: 20px;
          right: 45px;
          font-size: 60px;
          cursor: pointer;
        }
      </style>      
</head>
<body>
    <!-- The Overlay -->
    <div id="overlay" class="overlay">
        <!-- Overlay content -->
        <div class="overlay-content">
        <span class="overlay-close" onclick="closeOverlay()">&times;</span>
        <p>Please copy this ID and paste it into Prolific then close this box when done: <span style="color: red;" id="user_id_span"></span></p>
        </div>
    </div>
  
  
    <h1>Instructions</h1>
    
    <font size="+2">
        Thank you for being part of our data collection. We are part of the Brown University Humans To Robots Laboratory under Principal Investigator Prof. Stefanie Tellex. We’re gathering natural language commands for completing household tasks in AI2-THOR, a virtual environment.
        <br>
        <br>
        A personal assistant robot has the ability to navigate to different landmarks in the environment and manipulate objects. It also has a camera, so it can perceive different items and qualities about its environment. You will see a video below of how the robot operates in the environment.
        <br>
        <br>
        Your task is to provide <b>25</b> separate natural language commands. These 25 are split across 5 different scenes (environments), meaning you will provide 5 per scene. These <b>instruct the robot to perform tasks that include manipulation, navigation, and perception. To be super clear, all three must be present in the commands.</b> The tasks are limited to pick up/grab and place/drop tasks. Make sure the commands instruct the robot to move to different rooms. Examples of commands along with their video demos are shown below.
        <br>
        <br>
        The robot starts in the same position in each scene. That position depends on the specific scene. Each command in each scene should assume the robot starts in that position.  Each command you provide should specify a <b>single</b> task. The robot focuses on the current task and does not keep track of the previous tasks executed so far. 
        <br>
        <br>
        You can refer to the furniture and objects (excluding the ones from the list below) in the environment however you like as long as you are clear, the objects and rooms exist (excluding the ones from the list below), and the commands contain all three aspects (perception, navigation, manipulation), and have the robot moving to different rooms. The robot can come back to the starting room/position after it visits other rooms if you would like. The robot can take objects from the starting area/room to other areas/rooms if you would like. Don’t limit your commands to what you think the robot might “understand”, rather pretend the robot is as intelligent as a human.
        <br>
        <br>
        You will be able to reread these instructions and rewatch the videos in the following pages.
        <br>
        <br>
        After the 5th scene, you will receive the completion code.
        <br>
        <br>
        Do <b>NOT</b> refer to these objects, even if they exist in the scene:
        <ul>
            <li>Laptop</li>
            <li>Desk</li>
            <li>Anything that can be turned on/opened</li>
                <ul>
                    <li>Faucet</li>
                    <li>Lamp (sometimes labeled 'floor')</li>
                    <li>TV</li>   
                    <li>Drawers</li>
                    <li>Fridge</li>
                    <li>Etc.</li>
                </ul>
        </ul>
        <h4>Video:</h4>
        <iframe src="https://drive.google.com/file/d/1-POxqTh2AedW-ZIYSGbPJQc3b3FDfcOL/preview" width="640" height="480" allow="autoplay"></iframe>
        <h4>Examples:</h4>
        <ul>
            <li>Command: “Go to the living room and grab the purple pillow that’s on the couch, then go to the office and place it on the chair.” | <a href="https://drive.google.com/file/d/1sVPCb4crnW4ogTjP692hSDakNJmkYMUZ/view?usp=sharing" target="_blank">Demo video</a></li>
                <ul>
                    <li>Navigation: “Go to the living room”, “go to the office”</li>
                    <li>Manipulation: “grab”, “place it”</li>
                    <li>Perception: “purple pillow that’s on the couch”, “the chair”</li> 
                </ul>
            <li>Command: “Go to the living room and pick up the basketball, then go to the can and drop it inside.” | <a href="https://drive.google.com/file/d/13yvT4bc8vdUiDV7NC1cFB5bqHgmIFHtB/view?usp=sharing" target="_blank">Demo video</a></li>
                <ul>
                    <li>Navigation: “Go to the living room”, “go to the can”</li>
                    <li>Manipulation: “pick up”, “drop it”</li>
                    <li>Perception: “basketball”, “can”</li>
                </ul>
            <li>Command: “Go to the bedroom and grab my phone off the bed, then bring it back to me.” | <a href="https://drive.google.com/file/d/1IBgKx3n9BkmMyOuNUcuVzDjFugTd-2Pk/view?usp=sharing" target="_blank">Demo video</a></li>
                <ul>
                    <li>Navigation: “Go to the bedroom”, “bring it back to me”</li>
                    <li>Manipulation: “grab”</li>
                    <li>Perception: “my phone off the bed”, “me”</li>
                </ul>
          </ul> 
        <h4>FAQ:</h4>
        <ul>
            <li>Q: Can the robot search for an object rather than the command telling it exactly where it is? For example “Find the basketball and bring it back to me”.</li>
                <ul>
                    <li>A: Yes as long as the object is in another room.</li>
                </ul>
            <li>Q: Can I refer to specifics in the environment such in the commands such as the Vincent van Gogh painting on the wall?</li>
                <ul>
                    <li>A: Yes. Pretend the robot is as intelligent as a human and command it as you would a human.</li>
                </ul>
            <li>Q: Can I use both the passive and active voice for the commands?</li>
                <ul>
                    <li>A: Yes. The commands can have any style or tone.</li>
                </ul>
            <li>Q: What objects can I pick up?</li>
                <ul>
                    <li>A: The labeled ones in the AI2THOR demo website that a human can pick up with one hand. For example the TV is labeled in the website but it can’t be picked up by the robot.</li>
                </ul>
            <li>Q: In the AI2THOR demo I see that the objects have labels when I point on them, can I refer to objects that don’t have labels such as a desk?</li>
                <ul>
                    <li>A: Yes, as long as you’re not telling the robot to pick those objects up since those can’t be picked up. For example, you can tell the robot to place an object on a desk but you can’t pick up the desk.</li>
                </ul>
            <li>Q: Can I have the robot open objects such as drawers and turn on objects such as TVs?</li>
                <ul>
                    <li>A: No. While they can be opened in the demo website you’re exploring in, they can’t be opened in the actual simulator we’re going to be using. You are limited to pick up/grab and place/drop tasks.</li>
                </ul>
        </ul>
    </font>
    <br>
    <br>
    <br>
    <!-- Start button -->
    <button onclick="navigateToScene('scene1')" style="padding: 15px 30px; font-size: 20px; width: 200px; height: 50px;">Start</button>

    <script>
        window.onload = function() {
            document.getElementById('overlay').style.display = 'block';
            document.getElementById('user_id_span').innerText = "{{ user_id }}";
        };
    
        // Function to navigate to a scene
        function navigateToScene(sceneName) {
            window.location.href = '/' + sceneName;
        }
        
        // Function to close the overlay
        function closeOverlay() {
            document.getElementById('overlay').style.display = 'none';
        }
    </script>
    
    
    
      
</body>
</html>
