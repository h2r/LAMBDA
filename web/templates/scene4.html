<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scene 4</title>
    <style>
        #filtersImg {
            width: 500px;
            height: auto;
            position: relative;
            /* top: 50px;
            left: 100px; */
        }

        #mapImg {
            width: auto;
            height: 400px;
            position: relative;
        }

        .text-block {
            font-size: 1.5em; /* This sets the font size to +2 of the base font size */
            margin-bottom: 40px; /* Adds space between the text and the image */
        }

        #blocking-overlay {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 1);
            z-index: 9999;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        #blocking-message {
            color: white;
            font-size: 24px;
            text-align: center;
        }

        /* #next-button, #instructions-button, #verify-button { */
        #next-button, #instructions-button {
            background-color: #4CAF50;
            color: white;
            padding: 15px 32px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            cursor: pointer;
            margin-top: 20px;
            margin-right: 10px;
        }

        .visually-hidden {
            position: absolute;
            width: 1px;
            height: 1px;
            margin: -1px;
            padding: 0;
            overflow: hidden;
            clip: rect(0, 0, 0, 0);
            border: 0;
        }

        .visually-hidden-checkbox {
            position: absolute;
            opacity: 0; /* Makes it invisible */
            width: 1px;
            height: 1px;
            margin: -1px;
            padding: 0;
            overflow: hidden;
            clip: rect(0, 0, 0, 0);
            border: 0;
        }


        .form-container {
            margin-top: 20px;
        }

        .form-container label {
            display: block;
            margin-top: 10px;
        }

        .form-container input[type="text"] {
            width: 300px;
            padding: 5px;
            margin-bottom: 10px;
        }

        #instructions-modal {
            display: none;
            position: fixed;
            z-index: 10000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            overflow: auto;
            background-color: rgba(0, 0, 0, 0.4);
        }

        .modal-content {
            background-color: #fefefe;
            margin: 15% auto;
            padding: 20px;
            border: 1px solid #888;
            width: 50%;
            border-radius: 5px;
        }

        .close {
            color: #aaa;
            float: right;
            font-size: 28px;
            font-weight: bold;
            cursor: pointer;
        }
    </style>
</head>
<body>
    <h1>Scene 4</h1>

    <div id="blocking-overlay">
        <div id="blocking-message">Scene 4 loading... Will take approximately 15 seconds</div>
    </div>
    <div id="checkboxContainer"></div>

    <div class="text-block">
        1. The following steps are the same as before but with a different floorplan. Please go to the simulator below and unselect the filter checkboxes until only the <b>RoboTHOR Training</b> one is checked. It should look like the image below:
    </div>
    
    <img src="/static/filters.png" alt="Filters checkboxes" id="filtersImg">

    <div class="text-block">
        2. Then please scroll down in the simulator to all the scenes and select <b>Floorplan_Train8_2</b>. It looks like this:
    </div>

    <img src="/static/train8_2.png" alt="Train8_2" id="mapImg">

    <div class="text-block">
        3. Next, scroll back up to the loaded scene and explore it using the WASD keys to move and the mouse to point at objects to see their labels, pick up, and throw the objects. Explore the rooms and objects as much as possible that way brainstorming tasks becomes easier.
    </div>

    <div class="text-block">
        4. Finally, fill in all the command boxes for all the 5 commands you thought of for that scene. Then click <b>Next</b> to proceed to the next page.
    </div>

    <div class="text-block">
        <i>Tip: Click the <b>Instructions</b> button if you would like to review the instructions from the first page.</i>
    </div>

    <iframe id="myIframe" src="https://ai2thor.allenai.org/demo/" width="1400" height="800"></iframe>
    
    
    <div class="form-container">
        <form id="command-form">
            <label for="command1">Command 1:</label>
            <input type="text" id="command1" name="command1" class="command-input">
            <label for="command2">Command 2:</label>
            <input type="text" id="command2" name="command2" class="command-input">
            <label for="command3">Command 3:</label>
            <input type="text" id="command3" name="command3" class="command-input">
            <label for="command4">Command 4:</label>
            <input type="text" id="command4" name="command4" class="command-input">
            <label for="command5">Command 5:</label>
            <input type="text" id="command5" name="command5" class="command-input">
        </form>
    </div>

    <div id="submission-message" style="display: none;">Commands submitted!</div>

   
    <!-- <button id="verify-button" onclick="verify()" disabled>Verify</button> -->
    <button id="next-button" onclick="dataAndNextScene()" disabled>Next</button>
    <button id="instructions-button" onclick="showInstructions()">Instructions</button>

    <div id="instructions-modal">
        <div class="modal-content">
            <span class="close" onclick="closeInstructions()">&times;</span>
            <h1>Instructions</h1>
            <!-- Instructions content here -->
            <font size="+2">
                Thank you for being part of our data collection. We are part of the Brown University Humans To Robots Laboratory under Principal Investigator Prof. Stefanie Tellex. We’re gathering natural language commands for completing household tasks in AI2-THOR, a virtual environment.
                <br>
                <br>
                A personal assistant robot has the ability to navigate to different landmarks in the environment and manipulate objects. It also has a camera, so it can perceive different items and qualities about its environment. You will see a video below of how the robot operates in the environment.
                <br>
                <br>
                Your task is to provide <b>25</b> separate natural language commands. These 25 are split across 5 different scenes (environments), meaning you will provide 5 per scene. These <b>instruct the robot to perform tasks that include manipulation, navigation, and perception. To be super clear, all three must be present in the commands.</b> The tasks are limited to pick up/grab and place/drop tasks. Make sure the commands instruct the robot to move to different rooms. Examples of commands along with their video demos are shown below.
                <br>
                <br>
                The robot starts in the same position in each scene. That position depends on the specific scene. Each command in each scene should assume the robot starts in that position.  Each command you provide should specify a <b>single</b> task. The robot focuses on the current task and does not keep track of the previous tasks executed so far. 
                <br>
                <br>
                You can refer to the furniture and objects (excluding the ones from the list below) in the environment however you like as long as you are clear, the objects and rooms exist (excluding the ones from the list below), and the commands contain all three aspects (perception, navigation, manipulation), and have the robot moving to different rooms. The robot can come back to the starting room/position after it visits other rooms if you would like. The robot can take objects from the starting area/room to other areas/rooms if you would like. Don’t limit your commands to what you think the robot might “understand”, rather pretend the robot is as intelligent as a human.
                <br>
                <br>
                You will be able to reread these instructions and rewatch the videos in the following pages.
                <br>
                <br>
                After the 5th scene, you will receive the completion code.
                <br>
                <br>
                Do <b>NOT</b> refer to these objects, even if they exist in the scene:
                <ul>
                    <li>Laptop</li>
                    <li>Desk</li>
                    <li>Anything that can be turned on/opened</li>
                        <ul>
                            <li>Faucet</li>
                            <li>Lamp (sometimes labeled 'floor')</li>
                            <li>TV</li>   
                            <li>Drawers</li>
                            <li>Fridge</li>
                            <li>Etc.</li>
                        </ul>
                </ul>
                <h4>Video:</h4>
                <iframe src="https://drive.google.com/file/d/1-POxqTh2AedW-ZIYSGbPJQc3b3FDfcOL/preview" width="640" height="480" allow="autoplay"></iframe>
                <h4>Examples:</h4>
                <ul>
                    <li>Command: “Go to the living room and grab the purple pillow that’s on the couch, then go to the office and place it on the chair.” | <a href="https://drive.google.com/file/d/1sVPCb4crnW4ogTjP692hSDakNJmkYMUZ/view?usp=sharing" target="_blank">Demo video</a></li>
                        <ul>
                            <li>Navigation: “Go to the living room”, “go to the office”</li>
                            <li>Manipulation: “grab”, “place it”</li>
                            <li>Perception: “purple pillow that’s on the couch”, “the chair”</li> 
                        </ul>
                    <li>Command: “Go to the living room and pick up the basketball, then go to the can and drop it inside.” | <a href="https://drive.google.com/file/d/13yvT4bc8vdUiDV7NC1cFB5bqHgmIFHtB/view?usp=sharing" target="_blank">Demo video</a></li>
                        <ul>
                            <li>Navigation: “Go to the living room”, “go to the can”</li>
                            <li>Manipulation: “pick up”, “drop it”</li>
                            <li>Perception: “basketball”, “can”</li>
                        </ul>
                    <li>Command: “Go to the bedroom and grab my phone off the bed, then bring it back to me.” | <a href="https://drive.google.com/file/d/1IBgKx3n9BkmMyOuNUcuVzDjFugTd-2Pk/view?usp=sharing" target="_blank">Demo video</a></li>
                        <ul>
                            <li>Navigation: “Go to the bedroom”, “bring it back to me”</li>
                            <li>Manipulation: “grab”</li>
                            <li>Perception: “my phone off the bed”, “me”</li>
                        </ul>
                  </ul>
                <h4>FAQ:</h4>
                <ul>
                    <li>Q: Can the robot search for an object rather than the command telling it exactly where it is? For example “Find the basketball and bring it back to me”.</li>
                        <ul>
                            <li>A: Yes as long as the object is in another room.</li>
                        </ul>
                    <li>Q: Can I refer to specifics in the environment such in the commands such as the Vincent van Gogh painting on the wall?</li>
                        <ul>
                            <li>A: Yes. Pretend the robot is as intelligent as a human and command it as you would a human.</li>
                        </ul>
                    <li>Q: Can I use both the passive and active voice for the commands?</li>
                        <ul>
                            <li>A: Yes. The commands can have any style or tone.</li>
                        </ul>
                    <li>Q: What objects can I pick up?</li>
                        <ul>
                            <li>A: The labeled ones in the AI2THOR demo website that a human can pick up with one hand. For example the TV is labeled in the website but it can’t be picked up by the robot.</li>
                        </ul>
                    <li>Q: In the AI2THOR demo I see that the objects have labels when I point on them, can I refer to objects that don’t have labels such as a desk?</li>
                        <ul>
                            <li>A: Yes, as long as you’re not telling the robot to pick those objects up since those can’t be picked up. For example, you can tell the robot to place an object on a desk but you can’t pick up the desk.</li>
                        </ul>
                    <li>Q: Can I have the robot open objects such as drawers and turn on objects such as TVs?</li>
                        <ul>
                            <li>A: No. While they can be opened in the demo website you’re exploring in, they can’t be opened in the actual simulator we’re going to be using. You are limited to pick up/grab and place/drop tasks.</li>
                        </ul>
                </ul>
            </font>
        </div>
    </div>

    <script>
        function showBlockingOverlay() {
            var blockingOverlay = document.getElementById('blocking-overlay');
            blockingOverlay.style.display = 'flex';
        }

        function closeInstructions() {
            document.getElementById('instructions-modal').style.display = 'none';
        }

        function showInstructions() {
            document.getElementById('instructions-modal').style.display = 'block';
        }

        setTimeout(function() {
            var blockingOverlay = document.getElementById('blocking-overlay');
            blockingOverlay.style.display = 'none';
            var nextButton = document.getElementById('next-button');
            nextButton.style.display = 'block';
        }, 15000);

        function dataAndNextScene() {

            var formData = {
            'command1': document.getElementById('command1').value,
            'command2': document.getElementById('command2').value,
            'command3': document.getElementById('command3').value,
            'command4': document.getElementById('command4').value,
            'command5': document.getElementById('command5').value
            };

            // AJAX request to Flask
            fetch('/submit', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(formData)
            })
            .then(response => response.json())
            .then(data => {
                console.log(data.message);
                
                // Once data is submitted successfully, redirect to the next scene
                document.getElementById('submission-message').style.display = 'block';
                setTimeout(function() {
                    window.location.href = '/scene5';
                }, 1000);
            })
            .catch(error => console.error('Error:', error));
        }

        function checkInputs() {
            var inputs = document.getElementsByClassName('command-input');
            var allFilled = true;
            for (var i = 0; i < inputs.length; i++) {
                if (inputs[i].value === '') {
                    allFilled = false;
                    break;
                }
            }
            document.getElementById('next-button').disabled = !allFilled;
        }

        var inputs = document.getElementsByClassName('command-input');
        for (var i = 0; i < inputs.length; i++) {
            inputs[i].addEventListener('input', checkInputs);
        }

    </script>
</body>
</html>